{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797,
     "referenced_widgets": [
      "5677c996bf064b93bfb8ef77157ddc30",
      "e74c94fcd8164f45872120124613f8bd",
      "ee99a35a06f042a2a12585396bee37ea",
      "432b713f34064e5da23b64045a43b63b",
      "04c020fc26d14ce280f12a530bc81abc",
      "4002422de6534856b514bc0bb38bc38a",
      "77238d86aef948d083c39a275c90c8bf",
      "c3b6533d28ba48a6a37fd4d8b72545b2",
      "361d91b3d87b4e59bf59187cb148ddc8",
      "d85e4129e1b84b0692fad6fbdc51d747",
      "f67db11233fd451eaf5b6ebc9434f128",
      "61dabad8fcb24837b392b33f580ce226",
      "c7cf7b57b116403eb2a94988bdd2bba2",
      "b3bb9b01fad843f8a21b06785c883946",
      "2554a9bde0de4017a1616e999f1f15b5",
      "9e25f462ea6641c291d07abbceec20d1",
      "3d77941615164298b2f0ab27b2ff2395",
      "b71dd5077a2b419d83110bd5ef44f22d",
      "0f517504c7bd4e09835cbf3608eb6d2e",
      "dc4b1cb7ca4747fbaa028009759e092c",
      "8546dd650803431588a58feb0d221cbc",
      "674fe0b420b64675acf898052fd2bc19",
      "d044b4ed44eb4a9893c7bd4430208bf2",
      "963ecbe6bb4a48c6883df245fb6daf86",
      "61484e2b2a5946cd9fac0e0443b7a364",
      "d076b948181c4e5d9829bfbfbab31c1c",
      "5e28e75db1e546fb983675b3fca206dd",
      "a0d831abf35c43e59b608230eb732413",
      "b907e5a5168946ce8793861f780dc37f",
      "17c16f17fd264a36b05da1766ded52bb",
      "78e0195d16924343b13be51964d9487f",
      "b85454cc993a454ba9c4d7d73bd93a47",
      "a6e13731462c46beb610f1f811e66db9",
      "d95cdfce26dd41df89b658b50f4e4025",
      "df7bf0b4ecf34b8283ca8b8a3915e608",
      "9966412dc7b740639b53aff10ab580b3",
      "e7d7dc3d5a254612884c441316137870",
      "7be3b83a847e4d4b927c7d483d733233",
      "05915303efe44c2d9ecf479c447b0e96",
      "27f1bca8cd434ffd9a294cfb1d6bc8a0",
      "8a40fd2dd9ed46d298a1f7681a7380f2",
      "5a71d612ac114dfcb18c606014ecd383",
      "0c8c17874960499eaf5e8aa4fb840213",
      "5f31d0b03b3e4ebf963b88034517aa52",
      "8a223276d30d43f9b7adadc3b4dd69fa",
      "2429406cb6ae42958194c4eda7aab11f",
      "d723c5faed724b11bbcd257cb79e59e9",
      "528c8ba345fc420a8082c6e43c4aa017",
      "056a189fbfbb4ed19a82e15b47c28392",
      "1a8e4e16dc6e4752b997b42648911a8a",
      "42bc37d0793e45e1aab9d8b940593774",
      "04a83ecdc4d64326a5fd5482af6e78fc",
      "89e4b445d13d4f53a66a6f8aa3598dc5",
      "02e36e8a467941ce9c69435c1093a8cb",
      "c59415067c3a48a7b51ef47f61c786bc",
      "a0aec07245fb408cb4414772b4e157ec",
      "29bc8a12483b47fe95cc3e6176c0d54c",
      "b5f6f6c1bc3d46c19d8c28664c00bec9",
      "cc48ccca5a0440cdbbc0a148c508c7fa",
      "169600c3737f41e4b357e7759ae8a53c",
      "48beb6f5c36642c8862a6d623930b28c",
      "1a6395e1fb4d4854b612e4d56dec4ac8",
      "807c32a94cd04be696f762d5dd5ceed7",
      "e1858dce08b74634ac3eda5e14022f7e",
      "a8ca58dfd071447784b5836af2d8cf96",
      "a1bbef8e5aa44b84bb21ac0a3de5e9f5"
     ]
    },
    "executionInfo": {
     "elapsed": 65422,
     "status": "ok",
     "timestamp": 1760894474542,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "kqR0xnZDy1Mv",
    "outputId": "834e1c03-65f2-4222-c4e4-eb1d00034d7e"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LoRA-on-Teacher (PEFT) — paste & run as one cell in Colab\n",
    "# ----------------------------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Install libs (peft + hf)\n",
    "!pip install -q transformers datasets evaluate accelerate peft\n",
    "\n",
    "# Imports\n",
    "import os, gc, math, pprint, inspect, json\n",
    "import numpy as np, pandas as pd, torch\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- User knobs (edit if you want) ----------\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Colab Notebooks/HindiCodeMix\"\n",
    "SPLIT_BASE = \"/content/drive/MyDrive/Colab Notebooks/HindiCodeMix/data_processed\"\n",
    "DRIVE_BASE_LoRA = \"/content/drive/MyDrive/Colab Notebooks/LoRA/HindiCodeMix/DropOut_0.05\"\n",
    "TRAIN_CSV = os.path.join(SPLIT_BASE, \"train.csv\")\n",
    "VAL_CSV   = os.path.join(SPLIT_BASE, \"val.csv\")\n",
    "TEST_CSV  = os.path.join(SPLIT_BASE, \"test.csv\")\n",
    "\n",
    "# Where teacher full-finetuned model was saved by your teacher trainer\n",
    "TEACHER_SAVE_DIR = os.path.join(DRIVE_BASE, \"results_teacher_4epoch\", \"model\")\n",
    "# Output adapter folder (PEFT will save adapter files here)\n",
    "LORA_OUTPUT_DIR = os.path.join(DRIVE_BASE_LoRA, \"lora_adapter_r16_a64_epoch2\")\n",
    "\n",
    "# Base checkpoint id (fallback if TEACHER_SAVE_DIR is missing)\n",
    "CHECKPOINT = \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# LoRA hyperparams (sane defaults)\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_BIAS = \"lora_only\"   # \"none\" | \"all\" | \"lora_only\"\n",
    "\n",
    "# Training hyperparams\n",
    "MAX_LEN = 64             # match teacher trainer\n",
    "PER_DEVICE_BATCH = 8     # reduce if OOM; increase if VRAM allows\n",
    "GRAD_ACCUM = 2\n",
    "EPOCHS = 2\n",
    "LR = 2e-4                # LoRA typical: 1e-4 .. 5e-4 for many tasks\n",
    "FP16 = torch.cuda.is_available()\n",
    "\n",
    "# ---------- Load CSVs and build HF DatasetDict ----------\n",
    "for p in [TRAIN_CSV, VAL_CSV, TEST_CSV]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"CSV not found: {p} — run data-prep first\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "print(\"Sizes: train/val/test =\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "# Create HF DatasetDict (like teacher trainer)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "# ---------- Tokenizer (use same one saved with teacher) ----------\n",
    "TOKENIZER_PATH = os.path.join(os.path.dirname(TEACHER_SAVE_DIR), \"tokenizer\")\n",
    "print(\"Attempting to load tokenizer from:\", TOKENIZER_PATH)\n",
    "# robust load: use local_files_only to avoid hub validation\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True, local_files_only=True)\n",
    "    print(\"Loaded tokenizer from:\", TOKENIZER_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Local tokenizer load failed:\", e)\n",
    "    print(\"Falling back to HF checkpoint tokenizer:\", CHECKPOINT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT, use_fast=True)\n",
    "\n",
    "# safety: ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"review\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# re-add labels as ints (consistent with teacher trainer)\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", train_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"validation\"] = dataset[\"validation\"].add_column(\"label\", val_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", test_df[\"label\"].astype(int).tolist())\n",
    "\n",
    "# cast label to int64\n",
    "for s in [\"train\",\"validation\",\"test\"]:\n",
    "    dataset[s] = dataset[s].cast_column(\"label\", Value(\"int64\"))\n",
    "\n",
    "# set torch format\n",
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "if \"token_type_ids\" in dataset[\"train\"].column_names:\n",
    "    cols.insert(1, \"token_type_ids\")\n",
    "dataset.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ---------- Load base model (prefer teacher checkpoint if present) ----------\n",
    "base_model_source = TEACHER_SAVE_DIR if os.path.isdir(TEACHER_SAVE_DIR) else CHECKPOINT\n",
    "print(\"Loading base model from:\", base_model_source)\n",
    "base_config = AutoConfig.from_pretrained(base_model_source, num_labels=2, output_attentions=True, output_hidden_states=True)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_source, config=base_config).to(device)\n",
    "\n",
    "# ---------- Helper: auto-detect candidate target modules for LoRA ----------\n",
    "def detect_target_modules(model, substrings=None, max_matches=50):\n",
    "    \"\"\"\n",
    "    Finds module names that match common projection substrings and returns a list.\n",
    "    You can override by setting target_modules explicitly.\n",
    "    \"\"\"\n",
    "    if substrings is None:\n",
    "        substrings = [\"q_lin\",\"v_lin\",\"k_lin\",\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"c_attn\",\"c_proj\",\"query_key_value\",\"query\",\"key\",\"value\",\"c_attn\"]\n",
    "    names = []\n",
    "    for n, m in model.named_modules():\n",
    "        ln = n.lower()\n",
    "        for s in substrings:\n",
    "            if s in ln:\n",
    "                names.append(n)\n",
    "                break\n",
    "    # deduplicate while preserving order\n",
    "    seen = set(); res = []\n",
    "    for x in names:\n",
    "        if x not in seen:\n",
    "            seen.add(x); res.append(x)\n",
    "    print(f\"Detected {len(res)} candidate target modules (showing up to {max_matches}):\")\n",
    "    print(res[:max_matches])\n",
    "    return res\n",
    "\n",
    "candidate_targets = detect_target_modules(base_model)\n",
    "# If detection fails or returns nothing, fall back to a simple default (works for some HF models)\n",
    "if not candidate_targets:\n",
    "    candidate_targets = [\"query_key_value\", \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"c_attn\",\"c_proj\"]\n",
    "    print(\"Fallback target_modules:\", candidate_targets)\n",
    "\n",
    "# You can override target modules here if you know exact module names:\n",
    "target_modules = candidate_targets\n",
    "\n",
    "# ---------- Create LoraConfig and wrap model ----------\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=LORA_BIAS,\n",
    "    task_type=\"SEQ_CLS\"  # classification task\n",
    ")\n",
    "\n",
    "print(\"Wrapping base model with PEFT LoRA (this will freeze base params)...\")\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameter summary\n",
    "def print_trainable_summary(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total params: {total:,}, Trainable params (adapter): {trainable:,} ({trainable/total*100:.4f}%)\")\n",
    "print_trainable_summary(peft_model)\n",
    "\n",
    "# ---------------- Compatibility wrapper for TrainingArguments ----------------\n",
    "def make_train_args(output_dir, **kwargs):\n",
    "    \"\"\"\n",
    "    Create TrainingArguments robustly across HF versions:\n",
    "    maps 'evaluation_strategy' <-> 'eval_strategy' depending on signature.\n",
    "    \"\"\"\n",
    "    ta_kwargs = dict(kwargs)\n",
    "    sig = inspect.signature(TrainingArguments.__init__)\n",
    "    param_names = list(sig.parameters.keys())\n",
    "    # adapt evaluation_strategy name\n",
    "    if \"evaluation_strategy\" in ta_kwargs and \"evaluation_strategy\" not in param_names and \"eval_strategy\" in param_names:\n",
    "        ta_kwargs[\"eval_strategy\"] = ta_kwargs.pop(\"evaluation_strategy\")\n",
    "    # remove unsupported keys safely\n",
    "    if \"save_strategy\" in ta_kwargs and \"save_strategy\" not in param_names:\n",
    "        ta_kwargs.pop(\"save_strategy\", None)\n",
    "    if \"metric_for_best_model\" in ta_kwargs and \"metric_for_best_model\" not in param_names:\n",
    "        ta_kwargs.pop(\"metric_for_best_model\", None)\n",
    "    return TrainingArguments(output_dir=output_dir, **ta_kwargs)\n",
    "\n",
    "# ---------- TrainingArguments (LoRA) ----------\n",
    "out_dir = LORA_OUTPUT_DIR\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "train_args = make_train_args(\n",
    "    output_dir=out_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=FP16,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# small debug print to confirm settings\n",
    "print(\"TrainingArguments created. per_device_train_batch_size =\", train_args.per_device_train_batch_size)\n",
    "print(\"Num epochs =\", train_args.num_train_epochs)\n",
    "\n",
    "# ---------- Metrics (robust compute_metrics) ----------\n",
    "import numpy as _np\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Robust metric function that handles:\n",
    "     - logits as tuple (logits, other)\n",
    "     - logits as list of arrays (concatenate)\n",
    "     - logits as numpy array\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # if HF returned extra tuple values (e.g., (logits, hidden_states)), unwrap\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    # If list of arrays (batch-wise), concatenate along axis=0\n",
    "    if isinstance(logits, list):\n",
    "        try:\n",
    "            logits = _np.concatenate([_np.asarray(x) for x in logits], axis=0)\n",
    "        except Exception:\n",
    "            # fallback: try to take first element if shapes differ\n",
    "            logits = _np.asarray(logits[0])\n",
    "\n",
    "    # if torch tensor, convert\n",
    "    if hasattr(logits, \"detach\"):\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # final sanity: ensure logits is numpy array\n",
    "    logits = _np.asarray(logits)\n",
    "\n",
    "    if logits.ndim == 1:\n",
    "        # weird case: model predicted single logit per example; treat as prob and threshold 0.5\n",
    "        preds = (logits > 0.5).astype(\"int32\")\n",
    "    else:\n",
    "        preds = _np.argmax(logits, axis=-1)\n",
    "\n",
    "    # ensure labels is numpy array\n",
    "    if hasattr(labels, \"detach\"):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    labels = _np.asarray(labels)\n",
    "\n",
    "    # compute metrics\n",
    "    acc = float(accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"])\n",
    "    f1 = float(f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"])\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1}\n",
    "\n",
    "\n",
    "# ---------- Trainer ----------\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ---------- Train ----------\n",
    "print(\"Starting LoRA adapter training...\")\n",
    "trainer.train()\n",
    "\n",
    "# ---------- Save adapter (PEFT saves only adapter weights) ----------\n",
    "print(\"Saving LoRA adapter to:\", out_dir)\n",
    "trainer.model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(os.path.join(out_dir, \"tokenizer\"))  # optional\n",
    "\n",
    "# ---------- Evaluate adapter by loading back into base and running eval ----------\n",
    "print(\"Evaluating saved adapter...\")\n",
    "# load base model fresh (to avoid trainer wrappers), then load adapter\n",
    "base_for_eval = AutoModelForSequenceClassification.from_pretrained(base_model_source, config=base_config)\n",
    "peft_loaded = PeftModel.from_pretrained(base_for_eval, out_dir, is_trainable=False)\n",
    "peft_loaded.to(device)\n",
    "\n",
    "# tokenization for test (we used padding=max_length earlier)\n",
    "enc = tokenizer(test_df[\"review\"].astype(str).tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "indices = list(range(len(test_df)))\n",
    "batches = [indices[i:i+ (PER_DEVICE_BATCH*4) ] for i in range(0, len(indices), (PER_DEVICE_BATCH*4))]\n",
    "\n",
    "def eval_logits(model, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in batches:\n",
    "            out = model(input_ids=enc[\"input_ids\"][b].to(device), attention_mask=enc[\"attention_mask\"][b].to(device))\n",
    "            logits = out.logits if hasattr(out, \"logits\") else out[0]\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(test_df[\"label\"].astype(int).tolist()[b[0]:b[-1]+1])\n",
    "    acc = accuracy_metric.compute(predictions=all_preds, references=all_labels)[\"accuracy\"]\n",
    "    f1  = f1_metric.compute(predictions=all_preds, references=all_labels, average=\"macro\")[\"f1\"]\n",
    "    return float(acc), float(f1)\n",
    "\n",
    "teacher_total_params = sum(p.numel() for p in base_for_eval.parameters())\n",
    "adapter_trainable_params = sum(p.numel() for p in peft_loaded.parameters() if p.requires_grad)\n",
    "adapter_kb = adapter_trainable_params * 4 / 1024.0\n",
    "\n",
    "adapter_acc, adapter_f1 = eval_logits(peft_loaded, device)\n",
    "\n",
    "print(f\"Adapter eval -> Acc: {adapter_acc:.4f}, Macro-F1: {adapter_f1:.4f}\")\n",
    "print(f\"Adapter trainable params: {adapter_trainable_params:,} (~{adapter_kb:.1f} KB), Teacher total params: {teacher_total_params:,}\")\n",
    "\n",
    "# ---------- Save a tiny metadata JSON for result-summary script to pick up ----------\n",
    "meta = {\n",
    "    \"label\": os.path.basename(out_dir) or \"Teacher+LoRA\",\n",
    "    \"accuracy\": adapter_acc,\n",
    "    \"macro_f1\": adapter_f1,\n",
    "    \"adapter_trainable_params\": int(adapter_trainable_params),\n",
    "    \"adapter_kb\": float(adapter_kb),\n",
    "    \"teacher_params\": int(teacher_total_params),\n",
    "    \"notes\": f\"r={LORA_R},alpha={LORA_ALPHA},dropout={LORA_DROPOUT},bias={LORA_BIAS},lr={LR},epochs={EPOCHS}\"\n",
    "}\n",
    "with open(os.path.join(out_dir, \"adapter_results.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# cleanup\n",
    "try:\n",
    "    trainer.model.to(\"cpu\"); del trainer\n",
    "    peft_loaded.to(\"cpu\"); del peft_loaded\n",
    "    base_for_eval.to(\"cpu\"); del base_for_eval\n",
    "    base_model.to(\"cpu\"); del base_model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Done. Adapter saved at:\", out_dir)\n",
    "pprint.pprint(meta)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUS3L4X81RVYCMWeGsTOco",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
