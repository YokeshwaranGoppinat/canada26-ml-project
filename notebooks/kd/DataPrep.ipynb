{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK-qG5ykvVig"
   },
   "outputs": [],
   "source": [
    "# 0) Mount Drive + install deps\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UG7RFcra0U8f"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YdCg06cvmQW"
   },
   "outputs": [],
   "source": [
    "# 1) Imports & settings\n",
    "import os, glob, subprocess, gc, warnings\n",
    "import pandas as pd, numpy as np, torch, torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "import evaluate, transformers\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSBrCczDvsS5"
   },
   "outputs": [],
   "source": [
    "# ---------- User-editable knobs (make small for free Colab) ----------\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Colab Notebooks/CodeMix\"\n",
    "REPO_URL = \"https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset.git\"\n",
    "CLONE_PATH = os.path.join(DRIVE_BASE, \"repo\")\n",
    "FORCE_TRAIN_CSV = None   # optional: set explicit csv path in drive if autodetect fails\n",
    "LANG = \"tamil\"\n",
    "TASK = \"offensive\"\n",
    "SEED = 42\n",
    "\n",
    "RESULTS_DIR = os.path.join(DRIVE_BASE, \"results\")\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True); os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Clone repo if needed ----------\n",
    "if not os.path.exists(CLONE_PATH):\n",
    "    print(\"Cloning dataset repo...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, CLONE_PATH], check=True)\n",
    "else:\n",
    "    print(\"Repo present:\", CLONE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEbLI1h6xPwu"
   },
   "outputs": [],
   "source": [
    "# ---------- Find candidate CSV ----------\n",
    "def find_candidate_csv(lang, task):\n",
    "    patterns = [\n",
    "        os.path.join(CLONE_PATH, \"**\", f\"*{lang}*{task}*train*.csv\"),\n",
    "        os.path.join(CLONE_PATH, \"**\", f\"*{lang}*{task}*.csv\"),\n",
    "        os.path.join(CLONE_PATH, \"**\", f\"*{lang}*train*.csv\"),\n",
    "        os.path.join(CLONE_PATH, \"**\", f\"*{lang}*full*.csv\"),\n",
    "        os.path.join(CLONE_PATH, \"**\", f\"*{lang}*.csv\"),\n",
    "    ]\n",
    "    files=[]\n",
    "    for p in patterns: files.extend(glob.glob(p, recursive=True))\n",
    "    return sorted(list(set(files)))\n",
    "\n",
    "if FORCE_TRAIN_CSV:\n",
    "    train_csv = FORCE_TRAIN_CSV\n",
    "else:\n",
    "    candidates = find_candidate_csv(LANG, TASK)\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No dataset CSV found. Set FORCE_TRAIN_CSV to a path in Drive.\")\n",
    "    preferred = [f for f in candidates if \"full\" in f or \"train\" in f]\n",
    "    train_csv = preferred[0] if preferred else candidates[0]\n",
    "print(\"Using CSV:\", train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRuPOyQ2xYWd"
   },
   "outputs": [],
   "source": [
    "# ---------- Robust CSV read (comma or tab fallback) ----------\n",
    "def robust_read_csv(path):\n",
    "    for sep in [\",\",\"\\t\",\"|\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", on_bad_lines=\"skip\", engine=\"python\")\n",
    "            if df.shape[0] > 0 and df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: single-column read\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\", on_bad_lines=\"skip\", engine=\"python\")\n",
    "    return df\n",
    "\n",
    "df = robust_read_csv(train_csv)\n",
    "print(\"Loaded CSV shape:\", df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc2ZSLAFxk6h"
   },
   "outputs": [],
   "source": [
    "column_names = df.columns.tolist()\n",
    "print(column_names)\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "print(input_column)\n",
    "print(output_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6pCzrEMypdy"
   },
   "outputs": [],
   "source": [
    "print(df[output_column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRApIZauz1zx"
   },
   "outputs": [],
   "source": [
    "print(df[output_column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHhioI9-057z"
   },
   "outputs": [],
   "source": [
    "examples = []\n",
    "for index, row in df.iterrows():\n",
    "  input_value = row[input_column]\n",
    "  output_value = row[output_column]\n",
    "  if output_value == 'not-Tamil':\n",
    "    continue\n",
    "  label = None\n",
    "  if output_value == \"Not_offensive\":\n",
    "    label = output_value\n",
    "  elif output_value.startswith(\"Offensive\"):\n",
    "    label = \"Offensive\"\n",
    "  else:\n",
    "    raise ValueError(f\"Unexpected label: {output_value}\")\n",
    "  examples.append((input_value, label))\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5xBmGRl18Zu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels2count = Counter()\n",
    "for example in examples:\n",
    "  labels2count[example[1]] += 1\n",
    "print(labels2count)\n",
    "\n",
    "# Train: 3000, Val: 800, Test: 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdZY5V_k2VAn"
   },
   "outputs": [],
   "source": [
    "not_offensive_examples = []\n",
    "offensive_examples = []\n",
    "for example in examples:\n",
    "  if example[1] == \"Offensive\":\n",
    "    offensive_examples.append((example[0], 0))\n",
    "  else:\n",
    "    not_offensive_examples.append((example[0], 1))\n",
    "print(len(not_offensive_examples))\n",
    "print(len(offensive_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5l_G3DC2oCe"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "random.shuffle(not_offensive_examples)\n",
    "random.shuffle(offensive_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFofe_o12uh1"
   },
   "outputs": [],
   "source": [
    "num_train_per_class = 1500\n",
    "num_val_per_class = 400\n",
    "num_test_per_class = 400\n",
    "\n",
    "train_examples = not_offensive_examples[0:num_train_per_class] + offensive_examples[0:num_train_per_class]\n",
    "random.shuffle(train_examples)\n",
    "\n",
    "val_examples = not_offensive_examples[num_train_per_class:num_train_per_class+num_val_per_class] + offensive_examples[num_train_per_class:num_train_per_class+num_val_per_class]\n",
    "random.shuffle(val_examples)\n",
    "\n",
    "test_examples = not_offensive_examples[num_train_per_class+num_val_per_class:num_train_per_class+num_val_per_class+num_test_per_class] + offensive_examples[num_train_per_class+num_val_per_class:num_train_per_class+num_val_per_class+num_test_per_class]\n",
    "random.shuffle(test_examples)\n",
    "\n",
    "print(len(train_examples), len(val_examples), len(test_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T542nXXe36Do"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "train_filename = DRIVE_BASE + '/train.csv'\n",
    "\n",
    "with open(train_filename, 'w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  # Use writerows() to write all the data at once.\n",
    "  writer.writerows([['review', 'label']] + train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxgCbexu4nY4"
   },
   "outputs": [],
   "source": [
    "val_filename = DRIVE_BASE + '/val.csv'\n",
    "\n",
    "with open(val_filename, 'w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  # Use writerows() to write all the data at once.\n",
    "  writer.writerows([['review', 'label']] + val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBnGKY9X4r-Z"
   },
   "outputs": [],
   "source": [
    "test_filename = DRIVE_BASE + '/test.csv'\n",
    "\n",
    "with open(test_filename, 'w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  # Use writerows() to write all the data at once.\n",
    "  writer.writerows([['review', 'label']] + test_examples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMQ8pkfpNayXA3gCFh4LsQ3",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
