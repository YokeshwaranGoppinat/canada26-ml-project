{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687,
     "referenced_widgets": [
      "fd8ed31e24bd422a91db11d1dee54fda",
      "756e7e034a964c388939f9cb27c30366",
      "26cf1464edb74d0c95f4245d9900a6fc",
      "b8c8fb961ecb431fb21cf6b2ee6512cb",
      "f5791388b3d6420ca8a96e8c88fcba63",
      "be4e2bd5bfb4404684bae54b15f373fd",
      "17157de337624177a5911601d562994c",
      "75cdf976cf7c4368ba09ce386ff0f6b1",
      "eebd0c1f62fb4de2aaf79468248b27ab",
      "a5a777a065bd434cbed85210df1dd45b",
      "ea8c3e8dff26487b8d5d51469f822032",
      "e7d84ba396744148a01126f803104d6a",
      "a6bba5d27ce046cc8f76d36296738337",
      "0fe9435bd4744c7e8b8e20db8f77745e",
      "e4730f36f9b642c88c2f5e5331f8339f",
      "088ade85d4f8405da94d9eb74e55124c",
      "14bab4ec68094ed7a112c48d03b291b4",
      "7a4cae4571cc4c2aaf414ebcf6955755",
      "974831890cca489c8a19a9af67a51a6d",
      "c79d909d7e8f44a0aa1efe1c0c733aca",
      "093405df1625491b85d992cc6d3cc4b7",
      "4b483b80315f47d0b56896baac46b640",
      "0891beb1632a47f1bd372a953b7190cd",
      "b40e6c7ee5714494bc76c77b4ac439e4",
      "b71bd8fc49ae449ba788dbe8cd550b0a",
      "1273aa59268f43418eee103f4e6db843",
      "b6b381eb244c4877a885c1e195cdbd94",
      "daf10441532747748ca952a6b5cb6751",
      "b025b8267b4149d0a114a04366251e4d",
      "268d9ff2f06446088e0a65a0aa340a11",
      "7016030510724ffd85ab1a7f22ffb0d1",
      "a983e2c1384d4e8493b9edd12c7b4811",
      "450cfbe3d3d5449aa8ab9606b0d16585",
      "e2772c90067a48049eee71cfb25f828d",
      "dec6095e18bc449897da0fb19fdccc4b",
      "b1f692e6ad494f1eb5f458de62a1fd40",
      "1fc0620cca79426880cbd2f5135eb19e",
      "6bbba85f7aa1443c9dcb3ad9ab1bab61",
      "4d2584619329405eb72d6e7d2d1b9ee9",
      "c87e382ee07d48b097b5f06589c8f351",
      "f30d3d1551c34fb693ead6aded18329d",
      "36e3db97672e47c790f9d9ca7112ac46",
      "75bc6ff3138b429fbae8d16aa4c1139a",
      "9a44b2932e044b3b8e240a9699c06076",
      "d472277a5064451baff717c662b79d77",
      "98e886d416ca4cd5951eb23959fa2008",
      "57634073cd534dc791d32c73f193d356",
      "b77d1a8890f443bc889566112084d086",
      "8bbda0e43dbc4f6084080a3f99f15169",
      "9b3f425d97b94504a4aa3dc85128e0a3",
      "193137ae621d4581880205522718e466",
      "10fee48719404d3e9aba9bb7cbd9922f",
      "4f4f7d52a0c2460b8e1651e9e1b94550",
      "95a1ac9872eb470f86fadedea4c88ab8",
      "494da27e2f3e4f4b8271618c2312e62f",
      "98ba8b00971c4a76b913b3b0fcdaaf4c",
      "62c83f5ec73a4ddb99e0a76c7963cb57",
      "c2429197c09d4fa5b31bf6c8b22bc606",
      "be8a7c740595493f9d286eccd818b335",
      "433a2c279f914d43b5f1ed9e843bb34d",
      "6e48727efcbb4d8094c248337e07269b",
      "77fc370c83044d5ea002f389d73ba6ac",
      "e26b322f205d47ec9263377b8e353741",
      "ccb6517ee8014f8098c77c97f7fab54c",
      "30c968df057544339c112acabc7ef6bf",
      "a71f25341abe4b0780f3db6115e3f38c",
      "136f4d677b924d4cb2f1c0715109a9cc",
      "34b9041db28a4123ac6d2909fb91e0d6",
      "9138b1fed6084fae8ab1e7208d2496b9",
      "ae63666052c847dcbdc1c3e3ba7c0205",
      "d0415a26889841a1833ec6a6da5b116e",
      "92446861d0e540e6971423c6c2bcbdf4",
      "66a33db104ad476b97436ba4c9bc6e95",
      "1e3de37a50e744f8a148a6761a571881",
      "64bb07a741b04aaeb98b43c6a8276899",
      "c283b5c4ba924890b68caf9d5b89f974",
      "1d34bf90faee4f039c6160e0ce51c115",
      "cfe2df8769964bbfad96edbb0f1c14d2",
      "934b1ed5edd64902ab8bd29f5cce727c",
      "a59a22ffad454dd1ab8de7e8d957cd70",
      "2d48afbf0ea64f7992f937c7b98ad651",
      "34bf0617484c4d73864bfc300f77249a",
      "dbc32319a0a142ee8c9ef1d46ffca135",
      "6b52d61a53274c199b78b915c2ba1a93",
      "4a2e1ba1517a4b8cbc666b3c159ddccd",
      "8a251a8673f24fa4ab8abc573296712e",
      "7aa0163f453a486f87afcae16fb76ee3",
      "148a28fe5e22454f8966df7df08ce59a",
      "a2fa203af55d4d8eb1fec822782203e7",
      "575a47959c4543b795593b6b58919cb9",
      "7f3f366c06e844d883299303e6af5d44",
      "9418f6bcef854c4f972742d0561d38eb",
      "7cb5b26d43c04a998b4ca5b2025fe6b6",
      "f6d05e2ac9eb4edfa2d394e161e8eb92",
      "edca669ad5ed46259bbbc3aaf2c7a8b3",
      "3f11b1b9849c4473a7564d5a821240a7",
      "2c17870970f14209b9a7d83a37e274c3",
      "2437414c54c5407ea93eeba5291736c1",
      "2feb941a806b45229ca49291bb49fa0a",
      "665926108fe2400d961d3c6c9a45dd8e",
      "0d8e6fc659bb41d18882eb052418e920",
      "0aefbe261f4742018615b4f0308601d4",
      "8c66c9a3b6e54422ae57e009c767f1a6",
      "6a6607a0a28147908159674fdac8e276",
      "2c89efa9307243749a890361c54c8e22",
      "555c52f06fde4d8c951d11c08a30e473",
      "b93c1735391349f696cf03b8fb4e530d",
      "22a4e4f8e3ed4190b9d0bffffb58eb1d",
      "2b3acbad843044488239179ec94042aa",
      "c9debd7b67334591b76251b17f6900c3",
      "df16ca9ee7fe4eb0b6d9423093426318",
      "0c9f59f34e0c4d7c90a7e01c300f63ba",
      "f42acf2114dd40e6999fff864b9a5918",
      "40fa3b6253334a8baf6beb3967c3a4e8",
      "3cbc1ce40943416e89214a0903dae978",
      "fd85ea6b9b0f4e8fa775e7860b3e9083",
      "1e838ca72f66473cb3052c968e1e5506",
      "81567eef527e4b77ada86363fa4ea560",
      "357183b10a574c2992a5f31ce1189f4f",
      "6fca97cfb4774a39956fe62e492dc9a4",
      "676896a310d444d7bf2d4c5b237eb58e",
      "9e50e2c3595a4d04828c175c79321d5f",
      "fa3f2d0b32734805a34f601926ce871e",
      "2e443288f38f4b93bd5e4be55cbb183b",
      "4f12ec6446ea46258d7b9322230c32c7",
      "9af18939814f46d4a0ebc2fa3f2b549c",
      "f0dffeb07a0d470585dea344339c3957",
      "a907b9c0cfee4f798cbdda5d0fa01e30",
      "ce45a97eb5304aafbc1e725c399a4fcc",
      "f2a13f7f199f453b839a16d3d086db0c",
      "c1a85b8df5e9461aa8fe725078f6f5f7",
      "c009719daad2488a8726e9dddb428833"
     ]
    },
    "executionInfo": {
     "elapsed": 207766,
     "status": "ok",
     "timestamp": 1759283493194,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "vkfMHBdXM8d_",
    "outputId": "66e7ea1f-ad62-4ad0-bff5-a41a3f3bdf75"
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Student training (baseline, patched for 4-epoch teacher)\n",
    "# Saves results with \"_teacher4epoch\" in directory name\n",
    "# ===========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "# imports\n",
    "import os, gc, warnings, numpy as np, torch, pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "import evaluate\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# ---------- Paths ----------\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Colab Notebooks/CodeMix\"\n",
    "train_csv = os.path.join(DRIVE_BASE, \"train.csv\")\n",
    "val_csv   = os.path.join(DRIVE_BASE, \"val.csv\")\n",
    "test_csv  = os.path.join(DRIVE_BASE, \"test.csv\")\n",
    "\n",
    "# use new teacher directory\n",
    "teacher_base_dir = os.path.join(DRIVE_BASE, \"results_teacher_4epoch\")\n",
    "RESULTS_DIR = os.path.join(DRIVE_BASE, \"results_students\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# sanity checks\n",
    "for p in (train_csv, val_csv, test_csv):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing split file: {p}\")\n",
    "if not os.path.isdir(teacher_base_dir):\n",
    "    raise FileNotFoundError(f\"Teacher folder not found: {teacher_base_dir}\")\n",
    "\n",
    "# ---------- Teacher detection ----------\n",
    "def detect_teacher_folder(base_dir):\n",
    "    model_dir = os.path.join(base_dir, \"model\")\n",
    "    if os.path.isdir(model_dir) and \"config.json\" in os.listdir(model_dir):\n",
    "        return model_dir\n",
    "    if \"config.json\" in os.listdir(base_dir):\n",
    "        return base_dir\n",
    "    ckpts = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if d.startswith(\"checkpoint\")]\n",
    "    ckpts = [d for d in ckpts if os.path.isdir(d)]\n",
    "    if ckpts:\n",
    "        return max(ckpts, key=os.path.getmtime)\n",
    "    return None\n",
    "\n",
    "teacher_path = detect_teacher_folder(teacher_base_dir)\n",
    "if teacher_path is None:\n",
    "    raise FileNotFoundError(f\"Could not locate teacher model in {teacher_base_dir}\")\n",
    "print(\"Using teacher from:\", teacher_path)\n",
    "\n",
    "# ---------- Load dataset ----------\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "test_df  = pd.read_csv(test_csv)\n",
    "print(\"Loaded splits:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "CHECKPOINT = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"review\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "dataset = dataset.map(tokenize_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", train_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"validation\"] = dataset[\"validation\"].add_column(\"label\", val_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", test_df[\"label\"].astype(int).tolist())\n",
    "\n",
    "# keep expected cols\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "for split in dataset.keys():\n",
    "    to_remove = [c for c in dataset[split].column_names if c not in keep_cols]\n",
    "    if to_remove: dataset[split] = dataset[split].remove_columns(to_remove)\n",
    "    dataset[split] = dataset[split].cast_column(\"label\", Value(\"int64\"))\n",
    "dataset.set_format(type=\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "# ---------- Student Config ----------\n",
    "num_student_layers = 2   # adjust layer count here\n",
    "seed = 42\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    num_labels=2,\n",
    "    num_hidden_layers=num_student_layers,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "# baseline = no teacher guidance\n",
    "DISTILL_TYPE = \"baseline\"\n",
    "\n",
    "# clear any old globals\n",
    "for n in [\"trainer\",\"student\",\"teacher_loaded\"]:\n",
    "    if n in globals():\n",
    "        del globals()[n]\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- DistillTrainer ----------\n",
    "class DistillTrainer(Trainer):\n",
    "    def __init__(self, *args, distill_type=\"baseline\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.distill_type = distill_type\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- Training Args ----------\n",
    "PER_DEVICE_BATCH = 4\n",
    "GRAD_ACCUM = 2\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "\n",
    "# patched run name (with teacher4epoch marker)\n",
    "run_name = f\"student_{DISTILL_TYPE}_layers{num_student_layers}_seed{seed}_teacher4epoch\"\n",
    "output_dir = os.path.join(RESULTS_DIR, run_name)\n",
    "\n",
    "def make_train_args(output_dir, **kwargs):\n",
    "    ta_kwargs = dict(kwargs)\n",
    "    if \"evaluation_strategy\" in TrainingArguments.__init__.__code__.co_varnames:\n",
    "        if \"eval_strategy\" in ta_kwargs:\n",
    "            ta_kwargs[\"evaluation_strategy\"] = ta_kwargs.pop(\"eval_strategy\")\n",
    "    else:\n",
    "        if \"evaluation_strategy\" in ta_kwargs:\n",
    "            ta_kwargs[\"eval_strategy\"] = ta_kwargs.pop(\"evaluation_strategy\")\n",
    "    return TrainingArguments(output_dir=output_dir, **ta_kwargs)\n",
    "\n",
    "train_args = make_train_args(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student = AutoModelForSequenceClassification.from_config(student_config).to(device)\n",
    "\n",
    "trainer = DistillTrainer(\n",
    "    model=student,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    distill_type=DISTILL_TYPE\n",
    ")\n",
    "\n",
    "print(\"Starting baseline training | layers =\", num_student_layers)\n",
    "trainer.train()\n",
    "\n",
    "# ---------- Evaluate & Save ----------\n",
    "res = trainer.evaluate(dataset[\"test\"])\n",
    "print(\"Baseline student test results:\", res)\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "print(\"Saved baseline student ->\", output_dir)\n",
    "\n",
    "# cleanup\n",
    "trainer.model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_VfLvoPOwcE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxnxr1MHtdmIa/6343X+YC",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
