{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723,
     "referenced_widgets": [
      "4d970425d8d5497b87a8984dfa34b5e6",
      "dc5ae07ac39e44309a1fa50045c309f9",
      "a8e99c55b72e408d928d5602c92a5b56",
      "a17450154ddc41eb96cd6e230fb1f977",
      "43fb35f3104545abbcb9d02545446810",
      "7bc40c118ae7462b9d3f67eb9ce688d1",
      "b5d12969b672408a8f8e0e952cbb500f",
      "0e10326cb625455fbc97d7021ef58e60",
      "06304e72f17d4d4383bec28584f87703",
      "fdd6e27ab9f646329b8d5e11bc76c9f8",
      "7846204166d645f590f6308d2d254d28",
      "2528a9acdcd041a487d80b3e4c2e9061",
      "431059bd3fc449e4a8277ea2810cfa2b",
      "ab9490b9bbd84ee6836fe2292a556b46",
      "652fd7604c0c4045a28aaf18e6cff312",
      "2491368636534bd4950b2fe3020effdb",
      "eede389c015e4e508a42da79f549fe35",
      "63986267bb8a4ab7b45b9cb1627cbb38",
      "e6d6312cfac14845b8f165009c0070a8",
      "1df064eb3ec64151a41875708bd82baf",
      "ea368f8977a34cffb29bd387af77fcd6",
      "f930466e546a496cbeffe26269281093",
      "2b8978684e364d93873d5115001a20ea",
      "b5149df2b5734ff8b45977bc6fe9cbde",
      "a8b156f886af44d49e4f48d923d26e0c",
      "59c422dcac0b405abdf07a75014cc1cb",
      "581df80c59414620b86fc894baa31a75",
      "de1d7edd58964843ad8e3550c17b3641",
      "f5fa2acd6fc240b2b9138c9f73cfb019",
      "ee91190482d94cab911dd6ea23c69a90",
      "ca45db306ab74700a96823b43400fbfa",
      "8146ae2a93ad42929af023d861cdeac9",
      "dddf0e8936524249b2b9fc7a0ee7190e",
      "e9efe1f2b220492da9c33932bff5ea5b",
      "fcddfc4f10444aa78e0c0679f9f4c3bf",
      "8c29a6f53ef5485a9d1646913a97be17",
      "9810d47ca7a044b9868594ab55830feb",
      "e4b6695ca8794f21a2ceb0476fab3b88",
      "9771255f21da4e2784b842fc15266740",
      "e22db299ae1f4dc9ac6d1006dd2c6f7f",
      "3fb680e673174683b931133270698bee",
      "d34fbd06a58b43a38cbe5f698b09f63a",
      "b85061bf95c644eaae1403ea3cc025fa",
      "75617d05ea00409caf67603ec2be0087",
      "63a0670bdb3d41269d8e17d47ad4f97d",
      "8cbae971fdd047d78b19b0158f0b3e96",
      "82e26991c95841789a1640beebc2d8e2",
      "cf7dea1db8954135b22f52eecc754fdd",
      "806b64a589bf45e2bf5997f73562fbd4",
      "9153d3df72b0478c9d8fdd6604cbad93",
      "51fccf828f464e19ba6266bf653e1f1c",
      "49718ce2a71c49ab8d5efbc76d4f782a",
      "473afe0003054254a3d30ddeac3be763",
      "f764ebf069044011a88498cbe8cd7074",
      "79abc657716f44cdb961083df5b9177b",
      "016f44cd812c4177b00dbf973391bf78",
      "0d9ea485dcfc4d6a9a711790c9bd87de",
      "8cc7733e9851491ca0efa4c036e8dd40",
      "7fdc339fe8d44436bf6a47274277d255",
      "51afc79cf7134440a5f3f05d2148711e",
      "1d100a089bf2492887e727b38ce8a704",
      "45aff34a39a94730b417bd188fc0c59b",
      "ce0497fa10054e569780376ed12cbe3a",
      "774a7f9eecb7482bb14904eea6dd6e6b",
      "025d329aec594204b4d9b65bafa204b1",
      "efbcada420074c498e9cefd6f54b19b9",
      "21ef9692c0cd4f5daedfc2e9a4019e9c",
      "69f928312af74181a10ac8ffb2eae887",
      "e1b0f703bcc545979896011dc6f36e03",
      "b2f3c085c31245a1ab252c346e81e28b",
      "3f70897c80d042a2adae9e6888a5a40a",
      "a13e53d4b0464a18b06aa3e137ee9832",
      "1de58a4cf94e4b3ab6c8d2933ee752aa",
      "908a1c685cd041478612edbfbd93866e",
      "9852b6de1a47474a9238381c98c116c3",
      "50a470e8f84c4151ab70a3f8cf4ecb73",
      "310a7d3c619b4488996a863555eb1b59",
      "c5addfbffbbb4ee4900da3d35c476a3b",
      "6b184c6a9b314ba7912b279864dde066",
      "2a9c957598454af3bbfa733004d408ef",
      "51a0e1e7f70940f1963e3b5ab6173f38",
      "d38cfce10ac944f79c9ba73a530ff2b1",
      "40c3a24dc1494a819ee68556e1c9ac32",
      "105df821f91946938fd23dd9070912da",
      "eb5a8f8fa4bb429bb067cacdd7288616",
      "035b5f7125374ebf8ed1418dd8e05fe8",
      "32a6ee229e594fabb3847f9806175469",
      "b72981df83c4486b9aa1a7f873406808",
      "23272c61282141d0abba3634206362ae",
      "73b1f336618148648154cc2d79f5172c",
      "02815d91cd0b42d5a5cf75083589969c",
      "c7588dfe52e84f6ab02fa60f401eca07",
      "32d9fc05cae246338fa8d37e414c7dd1",
      "b175d96b79a647bf92ea9e5e1d7bf565",
      "a07405c1dc9f4b93b200610f09f129c5",
      "08ebe000dd054846812389f3ec9d7a69",
      "ba4ac5e2e40d47f481e13033c3c8ed73",
      "2a7f9bb70bed4bad9bd0f569665fa27e",
      "a63fb9fb2a0a40a3b1395599d3928eb2",
      "25d5326c86654540b4c778354d79c565",
      "5febb778094944b890638cd534e014eb",
      "d3dbc07f198046ddb2caa629d831091e",
      "9bfc235632b946d7b766f5ac77bc9428",
      "19f9d32dceb0494e91c5d192548a8ca0",
      "440a49b843db4d20a192d970b3e0b3a6",
      "575c16fbf1c046a0b73de73770946936",
      "e20b7259c2814cd4be91b48391dfcb4e",
      "08325fa6e3624f1cbdc54ea58b71e251",
      "502b5bc6fba448dcbc617198127a3529",
      "50014cb9445e43d1a7151686aa0888cf",
      "f11d72c6319940478fccb03875630e1e",
      "860ec8f1102a4528be133aaf953cb2c6",
      "b12f1abed68a480d9034b1988a23168f",
      "64f6b9b9fbe945f8991d3d035acf45dd",
      "0a449d46c03c4e7dbf5628a9523ee669",
      "be9a7aea9a954323851e1b818c1bd1f6",
      "7f162928c3ca484dae5816fa0d2c30c8",
      "9367cc2985864d548ce9e209072f48f4",
      "4e52fafa87984c9da519c74e4ee673e3",
      "5f1f7dd077484d5a81c3c3cb3b77e2a4",
      "2c5d555e010342778c551ab0ea6b3c59",
      "1be567660dae468d9f3f20ebe3dbb281",
      "d2dd1b0f96ad4c2fbeafd1ba0a8af3aa",
      "e9f616e79ed94f788dd92b8d77c908d1",
      "026447e8c9fe421db0b1121867209213",
      "2959fd6f1171462a85336be6473e2d12",
      "a0dacf92c0fb4487ab9efe5c36660259",
      "25b7f8ab6e8446cf9dda71e308de2039",
      "fda85091afae496fad84de0d5f9453c0",
      "9d24403e7a7940e09d12c9ebe4775d36",
      "c7b2e971bc4f410aaf13b3ddb2b92069",
      "24b7b4752281446b9210e50c3a113b5a"
     ]
    },
    "executionInfo": {
     "elapsed": 267145,
     "status": "ok",
     "timestamp": 1759215661347,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "vkfMHBdXM8d_",
    "outputId": "9d4f23fa-d960-4554-bf40-e12203422dae"
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Student training (embedding, 4epoch teacher) â€” patched\n",
    "# ===========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "# imports\n",
    "import os, gc, warnings, numpy as np, torch, torch.nn.functional as F, pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "import evaluate\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# paths (update if needed)\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Colab Notebooks/CodeMix\"\n",
    "train_csv = os.path.join(DRIVE_BASE, \"train.csv\")\n",
    "val_csv   = os.path.join(DRIVE_BASE, \"val.csv\")\n",
    "test_csv  = os.path.join(DRIVE_BASE, \"test.csv\")\n",
    "\n",
    "# ---- PATCHED: point to new teacher folder (4epoch) ----\n",
    "teacher_base_dir = os.path.join(DRIVE_BASE, \"results_teacher_4epoch\")\n",
    "RESULTS_DIR = os.path.join(DRIVE_BASE, \"results_students\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# simple checks\n",
    "for p in (train_csv, val_csv, test_csv):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing split file: {p}\")\n",
    "if not os.path.isdir(teacher_base_dir):\n",
    "    raise FileNotFoundError(f\"Teacher base folder not found: {teacher_base_dir}\")\n",
    "\n",
    "# detect teacher folder (prefer model/ then latest checkpoint)\n",
    "def detect_teacher_folder(base_dir):\n",
    "    model_dir = os.path.join(base_dir, \"model\")\n",
    "    if os.path.isdir(model_dir) and \"config.json\" in os.listdir(model_dir):\n",
    "        return model_dir\n",
    "    if \"config.json\" in os.listdir(base_dir) and any(n in os.listdir(base_dir)\n",
    "        for n in [\"pytorch_model.bin\",\"model.safetensors\"]):\n",
    "        return base_dir\n",
    "    ckpts = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if d.startswith(\"checkpoint\")]\n",
    "    ckpts = [d for d in ckpts if os.path.isdir(d)]\n",
    "    ckpts_with_model = [d for d in ckpts if any(n in os.listdir(d)\n",
    "        for n in [\"pytorch_model.bin\",\"model.safetensors\",\"config.json\"])]\n",
    "    if ckpts_with_model:\n",
    "        return max(ckpts_with_model, key=os.path.getmtime)\n",
    "    return None\n",
    "\n",
    "teacher_path = detect_teacher_folder(teacher_base_dir)\n",
    "if teacher_path is None:\n",
    "    raise FileNotFoundError(f\"Could not find teacher model in {teacher_base_dir}. Contents: {os.listdir(teacher_base_dir)}\")\n",
    "\n",
    "print(\"Using teacher folder:\", teacher_path)\n",
    "print(\"Files there:\", os.listdir(teacher_path)[:50])\n",
    "\n",
    "# load splits\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "test_df  = pd.read_csv(test_csv)\n",
    "print(\"Loaded splits sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "# tokenizer + tokenization (pad to fixed length)\n",
    "CHECKPOINT = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"review\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "dataset = dataset.map(tokenize_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", train_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"validation\"] = dataset[\"validation\"].add_column(\"label\", val_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", test_df[\"label\"].astype(int).tolist())\n",
    "\n",
    "# keep expected cols + cast labels\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "for split in dataset.keys():\n",
    "    to_remove = [c for c in dataset[split].column_names if c not in keep_cols]\n",
    "    if to_remove:\n",
    "        dataset[split] = dataset[split].remove_columns(to_remove)\n",
    "    dataset[split] = dataset[split].cast_column(\"label\", Value(\"int64\"))\n",
    "\n",
    "dataset.set_format(type=\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "# load teacher (local)\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_path,\n",
    "    local_files_only=True,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "teacher.eval()\n",
    "teacher.to(\"cpu\")\n",
    "print(\"Teacher loaded on:\", next(teacher.parameters()).device)\n",
    "\n",
    "# ---- PATCHED: set student layers + run metadata ----\n",
    "num_student_layers = 2   # change as desired\n",
    "seed = 42\n",
    "DISTILL_TYPE = \"embedding\"\n",
    "\n",
    "# student config\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    num_labels=2,\n",
    "    num_hidden_layers=num_student_layers,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "# clear old\n",
    "for n in [\"trainer\", \"trainer_student\", \"student\", \"teacher_loaded\"]:\n",
    "    if n in globals():\n",
    "        try: del globals()[n]\n",
    "        except: pass\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# DistillTrainer definition\n",
    "class DistillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, distill_type=\"baseline\", alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.distill_type = distill_type\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        if self.teacher is not None: self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        device = model.device\n",
    "        student_inputs = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in inputs.items()}\n",
    "        student_outputs = model(**student_inputs, output_hidden_states=True, output_attentions=True)\n",
    "        student_loss = student_outputs.loss\n",
    "\n",
    "        if self.distill_type == \"baseline\" or self.teacher is None:\n",
    "            return (student_loss, student_outputs) if return_outputs else student_loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_inputs = {k:(v.detach().cpu() if isinstance(v, torch.Tensor) else v) for k,v in inputs.items()}\n",
    "            teacher_outputs = self.teacher(**teacher_inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "        distill_loss = 0.0\n",
    "        if self.distill_type in (\"soft\", \"full\"):\n",
    "            t_logits = teacher_outputs.logits.to(device) / self.temperature\n",
    "            s_logits = student_outputs.logits / self.temperature\n",
    "            distill_loss += F.kl_div(\n",
    "                F.log_softmax(s_logits, dim=-1),\n",
    "                F.softmax(t_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (self.temperature ** 2)\n",
    "        if self.distill_type in (\"hidden\", \"full\"):\n",
    "            distill_loss += F.mse_loss(student_outputs.hidden_states[-1], teacher_outputs.hidden_states[-1].to(device))\n",
    "        if self.distill_type == \"embedding\":\n",
    "            distill_loss += F.mse_loss(student_outputs.hidden_states[0], teacher_outputs.hidden_states[0].to(device))\n",
    "        if self.distill_type == \"attention\":\n",
    "            distill_loss += F.mse_loss(\n",
    "                student_outputs.attentions[-1].sum(dim=1),\n",
    "                teacher_outputs.attentions[-1].sum(dim=1).to(device)\n",
    "            )\n",
    "\n",
    "        loss = self.alpha * student_loss + (1.0 - self.alpha) * distill_loss\n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "\n",
    "# Training args\n",
    "def make_train_args(output_dir, **kwargs):\n",
    "    ta_kwargs = dict(kwargs)\n",
    "    if \"evaluation_strategy\" in TrainingArguments.__init__.__code__.co_varnames:\n",
    "        if \"eval_strategy\" in ta_kwargs: ta_kwargs[\"evaluation_strategy\"] = ta_kwargs.pop(\"eval_strategy\")\n",
    "    else:\n",
    "        if \"evaluation_strategy\" in ta_kwargs: ta_kwargs[\"eval_strategy\"] = ta_kwargs.pop(\"evaluation_strategy\")\n",
    "    return TrainingArguments(output_dir=output_dir, **ta_kwargs)\n",
    "\n",
    "PER_DEVICE_BATCH = 4\n",
    "GRAD_ACCUM = 2\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "\n",
    "# ---- PATCHED: run name includes 4epoch tag ----\n",
    "run_name = f\"student_{DISTILL_TYPE}_layers{num_student_layers}_seed{seed}_teacher4epoch\"\n",
    "output_dir = os.path.join(RESULTS_DIR, run_name)\n",
    "# ------------------------------------------------\n",
    "\n",
    "train_args = make_train_args(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student = AutoModelForSequenceClassification.from_config(student_config).to(device)\n",
    "\n",
    "trainer = DistillTrainer(\n",
    "    model=student,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    teacher_model=teacher if DISTILL_TYPE != \"baseline\" else None,\n",
    "    distill_type=DISTILL_TYPE,\n",
    "    alpha=0.5,\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "print(\"Starting student training:\", run_name)\n",
    "trainer.train()\n",
    "\n",
    "# evaluate & save\n",
    "res = trainer.evaluate(dataset[\"test\"])\n",
    "print(\"Student test results:\", res)\n",
    "\n",
    "save_dir = output_dir\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved student ->\", save_dir)\n",
    "\n",
    "# cleanup\n",
    "trainer.model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1759215661357,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "0_VfLvoPOwcE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMAj1IVvBFhcIPzenriHeJa",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
