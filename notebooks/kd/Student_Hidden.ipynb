{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723,
     "referenced_widgets": [
      "57a5df60e6b24a3eab9be4e82c03ea98",
      "5e007e927452407db8382551ba7b71db",
      "e475245034a94e6bb5f080bfc0fe6795",
      "49a50f60121346fd944ada97417571e5",
      "5a752789a4b04b24993d80c1c33a6793",
      "fe3cfa42c7e742d38a246480d311a9e6",
      "e8bf7e510c3e4301a812d2e7d20aa0a4",
      "dae2cfde92e3446fafb6bb0f90a3a5b9",
      "bee7de1694904ee0b49a5828ce034bf2",
      "4f3980aaab4546ca9e6a0f2c6259b29c",
      "177e2b26c8814e12bc861f09e1f4acc2",
      "54ce0ad3cc884807a922421be56f0ed3",
      "81ef564fb2a54cc089fee5fb3bacb4c6",
      "90fe45f923eb4fb1999c1ff605271fa8",
      "f0bdfbf83db1452fb1dd31be48f2c845",
      "896529f907414577bf772b3b41aababc",
      "66cf56adf26c42c2899c119b56a84ead",
      "ef9b2ea1483f44c1bdad4a36fa893f25",
      "d435779c68c94867b5c39d235890827e",
      "2265870e14914eedbb3891795a0651fe",
      "225bc57ca3074e0aa8897e4cb8f7d9b9",
      "14ac7764bec14a30937943b5b4d4f344",
      "04b0b92b1d85467ba49680b8ad8a4f51",
      "b176fa99859a47ff8e938ba2b97a5da6",
      "53a7ee0bfc19453a85f174fa1d69e0a8",
      "542526d0362b438eb69e22ce24038420",
      "8512861ad3be49f78db5bafb968ee2dd",
      "234b4acfd8ab4e1e973a6fadb841c785",
      "da845c42705041e6a70bf23b83dd28a9",
      "bf68c191f25e460e8f75b8447ea4a9bd",
      "dd0992220ded472190e7444b2aeb189b",
      "d43cacf41c2c47fb86cf867a121d6f48",
      "5846acec4cc0404e9ca6b0bcd48ff96a",
      "680ac36df090451482473fdd465a5eaf",
      "3406742c95c44f6281851052b1712d3a",
      "60aee5bfd26a4ce890abd2314a495f98",
      "ef9684b833d045c2b57aecdf140af51a",
      "bb61b9fe6d8346169d75e051915d7c5f",
      "af82d256ca744cd593ab83dc474e6bae",
      "2ecd1905822a4d699c17fc53ba8eb4ea",
      "21c4e58f90b94a8bb159f8e9c4c3ba8b",
      "664ad32f37be45b2a6de3451c256f6d3",
      "b677e92cf3ce4c09b41d39e3e5f7e113",
      "c53424ce391c4c55b91b3dc12a725b6f",
      "ee5ad75917be433ba83dcec1f73d9b44",
      "39e518accc714fcfab0ca17227f247a7",
      "8e83979e01ca41ab9f95574c1c7886aa",
      "7623ce7a933f4f5e8583b13f35ac5aa8",
      "c06f232bd2174f42b0a6c1b601e7fd14",
      "a9aeac7f20954af99e036e0baf87b7ae",
      "9e290d2f2c48451f94cf357038078850",
      "1734c03bf97145dcb58564e56956ac0a",
      "16cffe89725e4a33bc37d8cbcbcccb49",
      "f49ebfd886e34111921e2c83778062e6",
      "fa8c39e8e9584402b5b02c72098cc38c",
      "72105d97458e41068d69097f2dd4c0cb",
      "879e4b756eda4422aca69f5260d8e946",
      "9e4540e3a15e4678b05763c86991ffb0",
      "4c68a9dbca0447d8865467d38cc7a6eb",
      "949213b8afdc49f7a03eb4bb7a159f5d",
      "1c05cddedcb74390a6634940b3ee4a67",
      "8518cdce5c7d4f6ca7464264fa53d5ad",
      "70545f474ef149d89bc22d3f432c21ab",
      "d85abffc64f244db9b416d5600c58153",
      "4e21dbf75cf6463886fe1e9be1f78cbc",
      "f4a293333f234207a2c439e5afd358fe",
      "e3c3d6766f4c46faa2ab517aa51b74d4",
      "ef5a90d6604c4f4eafff80f850e95269",
      "f4ae9e7e690a4182a52dbac578f1c412",
      "133b4fbb4c31456889cdef8732279438",
      "77f681574ca440148ee9d843efe663e4",
      "c76a839ee5d24f6e9fbbac7368efbefc",
      "f5c41b5d735543ae84cf05d5ce8ed6f0",
      "38bc340e62114435b0479d828c939efb",
      "c30ce2d21bf34f58ac64f4b326f7ce03",
      "7f9fa8282ca14efa9976f0de739d67af",
      "a8f69a33dcaa4c7383d3666f67d69545",
      "89224fb5c366450b882bd5e85fe6adaf",
      "9c1a35e142ef420885af176964e963b2",
      "ad8de739cac54504acdc9790be1a41fd",
      "717ec13b662d4f45ac0a096509afe2f9",
      "d3767fbe2d76404d965e269a0ef58f46",
      "d30c08c25807404298cc365620249377",
      "19588f52a332428195ea387b05683725",
      "0fc5fa90cb4d442a91bfddc27435a147",
      "c85043e19a044deda5623014cacc5b5c",
      "b8bd93ad409a4a0d85e6a6db7b0a1686",
      "190237156cb145e7840de7e5fa2dd4ae",
      "caebca7a8d9947fc933f5fb0471d339a",
      "268246bbaa7d4ee6a9994084d8fe332f",
      "12cca16714844db5b81a371a19a22253",
      "00606c5ee6cf4e209d03730dbe6c95d5",
      "dcffc42e6c42421594d3c3ec72ab94f7",
      "c83e082856994db2a986f8acd87d5f1d",
      "878b74c57a9b42d9832c96cf0be48d60",
      "c0f070c1e6b34e2d9512f1c2d24368de",
      "da6cc41a9c134a3dadb1194c79af7a62",
      "86cdb9b0672547ea988fad99cf21c339",
      "d4a988cf6208428ebe0e2cb0df0803bb",
      "ea3b684d146640f2b3d442fcfc3b3f60",
      "e9bb3e8c4ed8407dab81cfc8817d0d59",
      "abf6f562f21a41df8b3f997a127c8a38",
      "44c17d66dee743e5b5ed62d4770812e4",
      "9af223a67b3949f088af1df7575fd945",
      "70a5b20121484eccb56e38bbbb9d39fc",
      "0be8776e7ae542178a6569a264c60cc5",
      "2395d1d2fcf34fbcb7e8ce2ab4ebdc87",
      "5958f2737a564cf280a08da47132953b",
      "6e0207fb6da646bba0b3ceb5bfd992a8",
      "d3503d41c55540538990a678fbf5dbc4",
      "c58ae555bb7a48429b4b426a5dd38f86",
      "9838cc76747440a6a7f38e9fefbc563b",
      "a69d590173994b8c9ddeff32088668a2",
      "7afa0c1c7b3b49909c3b36a2eccaee99",
      "8c5d8d42069c4f9bbc336f15ded00610",
      "9688c34f29af499396e35ce461c4eddc",
      "d738aabb5ec341e0885ad3958a62f592",
      "322b6478c08040c88cb51f211c25e1c7",
      "1864c0f39df042e78c5233990032c38d",
      "1ee665a77dbe485396289e7118c67f81",
      "506cb148909d4801a8d13d8eae3ab1a5",
      "18e29c857513451ab5ce1291420a8e38",
      "0712e086e3704fd88dc0bcd17787bc59",
      "0f2639a8fdfd455085d87138cb7a204f",
      "854eaa28fc0c4921aab9672e00445b1c",
      "e1c8a7c9080048c8a90b9d21bcdfcb0d",
      "f8924c9af4524eeeac93d6e4fa421c27",
      "031334c7511640ad8f25663256f55034",
      "caab5c0e176b40c6a6753cdac3091bb7",
      "713ffa04d877494e884e74a76d9e3a73",
      "a6a38a3f8f3d4baab3b953d8b6be1bd1",
      "9fcc67b4c5c44e1ba57567e333624475"
     ]
    },
    "executionInfo": {
     "elapsed": 265218,
     "status": "ok",
     "timestamp": 1759214842118,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "vkfMHBdXM8d_",
    "outputId": "24a7444a-d5d7-485c-c464-d0fa673a33f8"
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Student training (hidden, 4epoch teacher) â€” patched\n",
    "# ===========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "# imports\n",
    "import os, gc, warnings, numpy as np, torch, torch.nn.functional as F, pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorWithPadding, set_seed\n",
    ")\n",
    "import evaluate\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# paths (update if needed)\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/Colab Notebooks/CodeMix\"\n",
    "train_csv = os.path.join(DRIVE_BASE, \"train.csv\")\n",
    "val_csv   = os.path.join(DRIVE_BASE, \"val.csv\")\n",
    "test_csv  = os.path.join(DRIVE_BASE, \"test.csv\")\n",
    "\n",
    "# ---- PATCHED: point to new teacher folder (4epoch) ----\n",
    "teacher_base_dir = os.path.join(DRIVE_BASE, \"results_teacher_4epoch\")\n",
    "RESULTS_DIR = os.path.join(DRIVE_BASE, \"results_students\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# simple checks\n",
    "for p in (train_csv, val_csv, test_csv):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Missing split file: {p}\")\n",
    "if not os.path.isdir(teacher_base_dir):\n",
    "    raise FileNotFoundError(f\"Teacher base folder not found: {teacher_base_dir}\")\n",
    "\n",
    "# detect teacher folder (prefer model/ then latest checkpoint)\n",
    "def detect_teacher_folder(base_dir):\n",
    "    model_dir = os.path.join(base_dir, \"model\")\n",
    "    if os.path.isdir(model_dir) and \"config.json\" in os.listdir(model_dir):\n",
    "        return model_dir\n",
    "    if \"config.json\" in os.listdir(base_dir) and any(n in os.listdir(base_dir)\n",
    "        for n in [\"pytorch_model.bin\",\"model.safetensors\"]):\n",
    "        return base_dir\n",
    "    ckpts = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if d.startswith(\"checkpoint\")]\n",
    "    ckpts = [d for d in ckpts if os.path.isdir(d)]\n",
    "    ckpts_with_model = [d for d in ckpts if any(n in os.listdir(d)\n",
    "        for n in [\"pytorch_model.bin\",\"model.safetensors\",\"config.json\"])]\n",
    "    if ckpts_with_model:\n",
    "        return max(ckpts_with_model, key=os.path.getmtime)\n",
    "    return None\n",
    "\n",
    "teacher_path = detect_teacher_folder(teacher_base_dir)\n",
    "if teacher_path is None:\n",
    "    raise FileNotFoundError(f\"Could not find teacher model in {teacher_base_dir}. Contents: {os.listdir(teacher_base_dir)}\")\n",
    "\n",
    "print(\"Using teacher folder:\", teacher_path)\n",
    "print(\"Files there:\", os.listdir(teacher_path)[:50])\n",
    "\n",
    "# load splits\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "test_df  = pd.read_csv(test_csv)\n",
    "print(\"Loaded splits sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "# tokenizer + tokenization (pad to fixed length)\n",
    "CHECKPOINT = \"distilbert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"review\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "dataset = dataset.map(tokenize_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset[\"train\"] = dataset[\"train\"].add_column(\"label\", train_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"validation\"] = dataset[\"validation\"].add_column(\"label\", val_df[\"label\"].astype(int).tolist())\n",
    "dataset[\"test\"] = dataset[\"test\"].add_column(\"label\", test_df[\"label\"].astype(int).tolist())\n",
    "\n",
    "# keep expected cols + cast labels\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "for split in dataset.keys():\n",
    "    to_remove = [c for c in dataset[split].column_names if c not in keep_cols]\n",
    "    if to_remove:\n",
    "        dataset[split] = dataset[split].remove_columns(to_remove)\n",
    "    dataset[split] = dataset[split].cast_column(\"label\", Value(\"int64\"))\n",
    "\n",
    "dataset.set_format(type=\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "# load teacher (local)\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_path,\n",
    "    local_files_only=True,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "teacher.eval()\n",
    "teacher.to(\"cpu\")\n",
    "print(\"Teacher loaded on:\", next(teacher.parameters()).device)\n",
    "\n",
    "# ---- PATCHED: set student layers + run metadata ----\n",
    "num_student_layers = 2   # change as desired\n",
    "seed = 42\n",
    "DISTILL_TYPE = \"hidden\"\n",
    "\n",
    "# student config\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    num_labels=2,\n",
    "    num_hidden_layers=num_student_layers,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "# clear old\n",
    "for n in [\"trainer\", \"trainer_student\", \"student\", \"teacher_loaded\"]:\n",
    "    if n in globals():\n",
    "        try: del globals()[n]\n",
    "        except: pass\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# DistillTrainer definition\n",
    "class DistillTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, distill_type=\"baseline\", alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.distill_type = distill_type\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        if self.teacher is not None: self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        device = model.device\n",
    "        student_inputs = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in inputs.items()}\n",
    "        student_outputs = model(**student_inputs, output_hidden_states=True, output_attentions=True)\n",
    "        student_loss = student_outputs.loss\n",
    "\n",
    "        if self.distill_type == \"baseline\" or self.teacher is None:\n",
    "            return (student_loss, student_outputs) if return_outputs else student_loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_inputs = {k:(v.detach().cpu() if isinstance(v, torch.Tensor) else v) for k,v in inputs.items()}\n",
    "            teacher_outputs = self.teacher(**teacher_inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "        distill_loss = 0.0\n",
    "        if self.distill_type in (\"soft\", \"full\"):\n",
    "            t_logits = teacher_outputs.logits.to(device) / self.temperature\n",
    "            s_logits = student_outputs.logits / self.temperature\n",
    "            distill_loss += F.kl_div(\n",
    "                F.log_softmax(s_logits, dim=-1),\n",
    "                F.softmax(t_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (self.temperature ** 2)\n",
    "        if self.distill_type in (\"hidden\", \"full\"):\n",
    "            distill_loss += F.mse_loss(student_outputs.hidden_states[-1], teacher_outputs.hidden_states[-1].to(device))\n",
    "        if self.distill_type == \"embedding\":\n",
    "            distill_loss += F.mse_loss(student_outputs.hidden_states[0], teacher_outputs.hidden_states[0].to(device))\n",
    "        if self.distill_type == \"attention\":\n",
    "            distill_loss += F.mse_loss(\n",
    "                student_outputs.attentions[-1].sum(dim=1),\n",
    "                teacher_outputs.attentions[-1].sum(dim=1).to(device)\n",
    "            )\n",
    "\n",
    "        loss = self.alpha * student_loss + (1.0 - self.alpha) * distill_loss\n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "\n",
    "# Training args\n",
    "def make_train_args(output_dir, **kwargs):\n",
    "    ta_kwargs = dict(kwargs)\n",
    "    if \"evaluation_strategy\" in TrainingArguments.__init__.__code__.co_varnames:\n",
    "        if \"eval_strategy\" in ta_kwargs: ta_kwargs[\"evaluation_strategy\"] = ta_kwargs.pop(\"eval_strategy\")\n",
    "    else:\n",
    "        if \"evaluation_strategy\" in ta_kwargs: ta_kwargs[\"eval_strategy\"] = ta_kwargs.pop(\"evaluation_strategy\")\n",
    "    return TrainingArguments(output_dir=output_dir, **ta_kwargs)\n",
    "\n",
    "PER_DEVICE_BATCH = 4\n",
    "GRAD_ACCUM = 2\n",
    "EPOCHS = 2\n",
    "LR = 2e-5\n",
    "\n",
    "# ---- PATCHED: run name includes 4epoch tag ----\n",
    "run_name = f\"student_{DISTILL_TYPE}_layers{num_student_layers}_seed{seed}_teacher4epoch\"\n",
    "output_dir = os.path.join(RESULTS_DIR, run_name)\n",
    "# ------------------------------------------------\n",
    "\n",
    "train_args = make_train_args(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student = AutoModelForSequenceClassification.from_config(student_config).to(device)\n",
    "\n",
    "trainer = DistillTrainer(\n",
    "    model=student,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    teacher_model=teacher if DISTILL_TYPE != \"baseline\" else None,\n",
    "    distill_type=DISTILL_TYPE,\n",
    "    alpha=0.5,\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "print(\"Starting student training:\", run_name)\n",
    "trainer.train()\n",
    "\n",
    "# evaluate & save\n",
    "res = trainer.evaluate(dataset[\"test\"])\n",
    "print(\"Student test results:\", res)\n",
    "\n",
    "save_dir = output_dir\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved student ->\", save_dir)\n",
    "\n",
    "# cleanup\n",
    "trainer.model.to(\"cpu\")\n",
    "gc.collect(); torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759214842124,
     "user": {
      "displayName": "Yokeshwaran Goppinat",
      "userId": "14819290310669589320"
     },
     "user_tz": -330
    },
    "id": "0_VfLvoPOwcE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZqjbcVjZSGHFzm6d4Ylpf",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
